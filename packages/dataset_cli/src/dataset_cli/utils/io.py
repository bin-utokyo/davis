# ./src/dataset_cli/src/dataset_cli/utils/io.py

import datetime
import hashlib
import mimetypes
from pathlib import Path

import charset_normalizer
import yaml
from pydantic import BaseModel, HttpUrl, ValidationError
from rich import print as rprint
from rich.progress import track

from dataset_cli.schemas import DatasetConfig, Manifest
from dataset_cli.schemas.manifest import DatasetInfo, PdfUrls


def parse_yaml_and_validate[T: BaseModel](
    yaml_path: Path,
    pydantic_model_class: type[T],
) -> T:
    """
    æŒ‡å®šã•ã‚ŒãŸYAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã€æŒ‡å®šã•ã‚ŒãŸPydanticãƒ¢ãƒ‡ãƒ«ã§æ¤œè¨¼ã—ã¾ã™ã€‚
    """
    if not yaml_path.exists():
        msg = f"YAMLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {yaml_path}"
        raise FileNotFoundError(msg)
    if not yaml_path.is_file() or yaml_path.suffix.lower() not in (".yaml", ".yml"):
        msg = f"æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã¯YAMLãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {yaml_path}"
        raise ValueError(msg)

    try:
        with yaml_path.open("r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
        return pydantic_model_class.model_validate(data)
    except yaml.YAMLError as e:
        msg = f"YAMLã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {yaml_path}"
        raise RuntimeError(msg) from e
    except ValidationError as e:
        rprint(f"[bold red]ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼ã‚¨ãƒ©ãƒ¼ in {yaml_path}:[/bold red]")
        rprint(e)
        raise  # TRY201: Use raise without specifying exception name


def generate_file_hash(file_path: Path) -> str:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã®SHA-256ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    sha256 = hashlib.sha256()
    with file_path.open("rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            sha256.update(chunk)
    return sha256.hexdigest().lower()


def infer_file_type(file_path: str | Path) -> str | None:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¿ã‚¤ãƒ—ã‚’æ¨æ¸¬ã—ã¾ã™ã€‚

    Args
    ----
    file_path: str | Path
        ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚

    Returns
    -------
    str: æ¨æ¸¬ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ— (ä¾‹: "csv", "json", "yaml").
    """
    file_type, _ = mimetypes.guess_type(file_path)
    return file_type


def detect_encoding(file_path: str, nbytes: int = 4096) -> str:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•åˆ¤å®šã™ã‚‹ã€‚
    """
    with Path(file_path).open("rb") as f:
        raw = f.read(nbytes)
    result = charset_normalizer.from_bytes(raw).best()
    if result is not None:
        return result.encoding
    return "utf-8"


def generate_manifest_data(
    cli_version: str,
    bootstrap_url: str,
    repo_url: str,
    branch: str,
) -> Manifest:
    """
    ãƒªãƒã‚¸ãƒªå†…ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã€Manifestã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    rprint("ğŸ” [bold]ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ã„ã¾ã™...[/bold]")
    data_root = Path("data")
    if not data_root.is_dir():
        msg = "'data' ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ«ãƒ¼ãƒˆã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        raise FileNotFoundError(msg)

    dvc_files = sorted(data_root.rglob("*.dvc"))
    rprint(f"  - {len(dvc_files)} å€‹ã® .dvc ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚")

    datasets: dict[str, DatasetInfo] = {}

    for dvc_file in track(dvc_files, description="ã‚¹ã‚­ãƒ¼ãƒã¨URLã‚’å‡¦ç†ä¸­..."):
        original_file = dvc_file.with_suffix("")
        schema_file = dvc_file.with_suffix(".schema.yaml")

        if not schema_file.exists():
            rprint(
                f"[yellow]è­¦å‘Š: ã‚¹ã‚­ãƒ¼ãƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {schema_file}ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚[/yellow]",
            )
            continue

        try:
            schema_config = parse_yaml_and_validate(schema_file, DatasetConfig)
        except (FileNotFoundError, ValueError, RuntimeError, ValidationError):
            continue

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDã¯ 'data/' ã‚’é™¤ã„ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
        dataset_id = original_file.parent.relative_to(data_root).as_posix()

        if dataset_id not in datasets:
            datasets[dataset_id] = DatasetInfo(
                name=schema_config.name,
                description=schema_config.description,
                year=schema_config.year,
                dvc_files=[],
            )

        datasets[dataset_id].dvc_files.append(dvc_file.as_posix())

        # PDFã¸ã®URLã‚’æ§‹ç¯‰
        pdf_ja_path = original_file.with_suffix(original_file.suffix + ".ja.pdf")
        pdf_en_path = original_file.with_suffix(original_file.suffix + ".en.pdf")
        if pdf_ja_path.exists() and pdf_en_path.exists():
            pdf_base_url = f"{repo_url}/blob/{branch}"
            pdf_url_ja, pdf_url_en = (
                HttpUrl(f"{pdf_base_url}/{pdf_ja_path.as_posix()}"),
                HttpUrl(f"{pdf_base_url}/{pdf_en_path.as_posix()}"),
            )
            datasets[dataset_id].pdf_urls[original_file.name] = PdfUrls(
                ja=pdf_url_ja,
                en=pdf_url_en,
            )

    return Manifest(
        manifest_version="1.1",
        cli_version=cli_version,
        generated_at=datetime.datetime.now(datetime.UTC),
        bootstrap_package_url=HttpUrl(bootstrap_url),
        datasets=datasets,
    )
