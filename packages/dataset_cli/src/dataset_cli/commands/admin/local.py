import datetime
import itertools as it
import json
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Annotated

import polars as pl
import typer
import yaml
from git import GitCommandError, InvalidGitRepositoryError, Repo
from pydantic import ValidationError
from rich import print as rprint
from rich.progress import track

from dataset_cli.schemas.dataset_config import (
    ColumnConfig,
    DatasetConfig,
    LocalizedStr,
)
from dataset_cli.schemas.polars import get_polars_data_type_name
from dataset_cli.utils.dvc import DVCClient
from dataset_cli.utils.parser import infer_file_type
from dataset_cli.utils.validate import (
    detect_encoding,
    read_data_with_schema,
    validate_paths,
)

app = typer.Typer(no_args_is_help=True, help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ã‚«ãƒ«ç®¡ç†ã‚³ãƒžãƒ³ãƒ‰")


DVC_EXCLUDE_SUFFIXES = {".dvc", ".schema.yaml", ".pdf", ".DS_Store", ".gitignore"}
VALIDATION_TARGET_EXTENSIONS = {".csv", ".xlsx", ".xls", ".json", ".parquet"}
PDF_TARGET_EXTENSIONS = {".csv", ".xlsx", ".xls"}


def _get_and_validate_repo() -> Repo:
    try:
        repo = Repo(search_parent_directories=True)
    except InvalidGitRepositoryError as e:
        rprint("[bold red]ã‚¨ãƒ©ãƒ¼: ã“ã“ã¯Gitãƒªãƒã‚¸ãƒˆãƒªã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚[/bold red]")
        raise typer.Exit(code=1) from e
    return repo


def _get_dvc_deleted_files(dvc_client: DVCClient) -> list[str]:
    try:
        result = dvc_client.status("--quiet", "--json")
        if not result:
            return []
        status = json.loads(result)
        return [item["path"] for item in status.get("deleted", [])]
    except json.JSONDecodeError:
        return []


def _is_excluded(path: Path) -> bool:
    return any(str(path).endswith(suffix) for suffix in DVC_EXCLUDE_SUFFIXES)


def _detect_changes(repo: Repo, repo_path: Path, dvc_client: DVCClient) -> list[Path]:
    rprint("\n[bold]Step 1: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¤‰æ›´ã‚’æ¤œå‡ºã—ã¾ã™...[/bold]")
    deleted_files = _get_dvc_deleted_files(dvc_client)
    if deleted_files:
        rprint(f"  [red]å‰Šé™¤ã‚’æ¤œå‡º (DVC):[/red] {', '.join(deleted_files)}")
        dvc_files_to_remove = [f"{f}.dvc" for f in deleted_files]
        if dvc_files_to_remove:
            repo.index.remove(dvc_files_to_remove, working_tree=True, r=True)
            rprint(
                f"  [red]å¯¾å¿œã™ã‚‹.dvcãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤:[/red] {', '.join(dvc_files_to_remove)}",
            )

    data_dir = repo_path / "data"
    all_data_files = (
        [
            p.relative_to(repo_path)
            for p in data_dir.rglob("*")
            if p.is_file() and not _is_excluded(p)
        ]
        if data_dir.is_dir()
        else []
    )

    if not all_data_files:
        rprint(
            "  [dim]data/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«DVCã§ç®¡ç†ã™ã‚‹å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚[/dim]",
        )
    else:
        rprint(
            f"  [cyan]DVCç®¡ç†å€™è£œ (ã‚¹ã‚­ãƒ£ãƒ³):[/cyan] {len(all_data_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«",
        )
    return all_data_files


def _dvc_add_and_validate(
    repo_path: Path, files: list[Path], dvc_client: DVCClient
) -> None:
    rprint("\n[bold]Step 2: DVCã¸ã®è¿½åŠ ã¨ã‚¹ã‚­ãƒ¼ãƒžæ¤œè¨¼ã‚’é–‹å§‹ã—ã¾ã™...[/bold]")
    if not files:
        return

    files_to_add_str = [str(p) for p in files]
    dvc_client.add(files_to_add_str)
    rprint("  [cyan]âœ“[/cyan] `dvc add` ã‚’å®Ÿè¡Œã—ã€DVCã®è¿½è·¡æƒ…å ±ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")

    files_for_validation_check = [
        f for f in files if f.suffix in VALIDATION_TARGET_EXTENSIONS
    ]
    validation_targets: set[Path] = set()
    processed_dirs: set[Path] = set()

    for file_path_rel in files_for_validation_check:
        file_path = repo_path / file_path_rel
        file_schema = file_path.with_suffix(file_path.suffix + ".schema.yaml")
        if file_schema.exists():
            validation_targets.add(file_path_rel)
            continue
        dir_path = file_path.parent
        if dir_path in processed_dirs:
            continue
        dir_schema = dir_path / "schema.yaml"
        if dir_schema.exists():
            validation_targets.add(dir_path.relative_to(repo_path))
            processed_dirs.add(dir_path)

    if not validation_targets:
        return

    rprint(f"  [cyan]æ¤œè¨¼ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ:[/cyan] {[str(p) for p in validation_targets]}")
    has_error = False
    for target_path in validation_targets:
        try:
            validate((repo_path / target_path).as_posix())
        except typer.Exit:
            has_error = True
    if has_error:
        rprint(
            "\n[bold red]ã‚¨ãƒ©ãƒ¼: 1ã¤ä»¥ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚åŒæœŸã‚’ä¸­æ–­ã—ã¾ã™ã€‚[/bold red]",
        )
        raise typer.Exit(code=1)
    rprint("[green]âœ… æ¤œè¨¼å¯¾è±¡ã¯ã™ã¹ã¦æ¤œè¨¼ã‚’é€šéŽã—ã¾ã—ãŸã€‚[/green]")


def _generate_pdfs_and_stage(repo: Repo, repo_path: Path) -> None:
    rprint("\n[bold]Step 3: PDFç”Ÿæˆã¨æœ€çµ‚ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ã‚’è¡Œã„ã¾ã™...[/bold]")
    generate_pdf(repo_path.as_posix())
    repo.git.add(all=True)
    rprint("  [green]âœ“[/green] å…¨ã¦ã®å¤‰æ›´ã‚’Gitã«ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ã—ã¾ã—ãŸã€‚")


def _commit_and_push(
    repo: Repo, repo_path: Path, message: str, dvc_client: DVCClient
) -> None:
    rprint("\n[bold]Step 4: ã‚³ãƒŸãƒƒãƒˆã¨ãƒ—ãƒƒã‚·ãƒ¥ã‚’è¡Œã„ã¾ã™...[/bold]")
    if not repo.index.diff(repo.head.commit):
        rprint(
            "\n[green]âœ… ã‚³ãƒŸãƒƒãƒˆã™ã¹ãå¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚[/green]",
        )
        return

    final_commit_message = message or typer.prompt(
        "ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
        default="Update dataset",
    )
    repo.index.commit(final_commit_message)
    rprint(f"[green]âœ… Gitã«ã‚³ãƒŸãƒƒãƒˆã—ã¾ã—ãŸ: '{final_commit_message}'[/green]")

    if typer.confirm("\nDVC ã¨ Git ã®å¤‰æ›´ã‚’ãƒªãƒ¢ãƒ¼ãƒˆã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¾ã™ã‹ï¼Ÿ", default=True):
        dvc_client.push()
        current_branch = repo.active_branch
        if current_branch.tracking_branch() is None:
            rprint(
                "  [dim]ä¸Šæµãƒ–ãƒ©ãƒ³ãƒãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚'--set-upstream'ã‚’ã¤ã‘ã¦ãƒ—ãƒƒã‚·ãƒ¥ã—ã¾ã™...[/dim]",
            )
            repo.git.push("--set-upstream", "origin", current_branch.name)
        else:
            repo.remotes.origin.push()
        rprint("[green]ðŸš€ âœ… DVCã¨Gitã®ãƒªãƒ¢ãƒ¼ãƒˆåŒæœŸãŒå®Œäº†ã—ã¾ã—ãŸï¼[/green]")
    else:
        rprint("[yellow]ãƒ—ãƒƒã‚·ãƒ¥ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚[/yellow]")


@app.command(
    name="sync",
    help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¤‰æ›´ã‚’æ¤œè¨¼ã—ã€DVCã¨Gitã«ã‚³ãƒŸãƒƒãƒˆãƒ»ãƒ—ãƒƒã‚·ãƒ¥ã—ã¾ã™ã€‚",
)
def sync_dataset(
    commit_message: Annotated[
        str,
        typer.Option("--message", "-m", help="ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‚"),
    ] = "",
) -> None:
    repo = _get_and_validate_repo()
    repo_path = Path(repo.working_dir)
    dvc_client = DVCClient(repo_path=repo_path)
    try:
        changed_files = _detect_changes(repo, repo_path, dvc_client)
        _dvc_add_and_validate(repo_path, changed_files, dvc_client)
        _generate_pdfs_and_stage(repo, repo_path)
        _commit_and_push(repo, repo_path, commit_message, dvc_client)
    except (
        GitCommandError,
        subprocess.CalledProcessError,
        FileNotFoundError,
        OSError,
    ) as e:
        rprint(f"[bold red]Git/DVCã‚³ãƒžãƒ³ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}[/bold red]")


# --- validate command --- #
@app.command(
    name="validate",
    help="æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒ¼ãƒžã«ç…§ã‚‰ã—ã¦æ¤œè¨¼ã—ã¾ã™ã€‚",
)
def validate(
    file_or_dir_path: Annotated[
        str,
        typer.Argument(help="æ¤œè¨¼ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹"),
    ],
    schema_path_str: Annotated[
        str | None,
        typer.Option("--schema", "-s", help="æ¤œè¨¼ã«ä½¿ç”¨ã™ã‚‹ã‚¹ã‚­ãƒ¼ãƒžãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹"),
    ] = None,
    encoding: Annotated[
        str | None,
        typer.Option(help="ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆä¾‹: utf-8, cp932 ãªã©ï¼‰"),
    ] = None,
) -> None:
    path_obj = Path(file_or_dir_path)
    if not path_obj.exists():
        rprint(f"[red]æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {file_or_dir_path}[/red]")
        raise typer.Exit(code=1)

    if path_obj.is_file():
        _validate_file(file_or_dir_path, schema_path_str, encoding)
    elif path_obj.is_dir():
        _validate_directory(file_or_dir_path, schema_path_str, encoding)
    else:
        rprint(f"[red]ä¸æ˜Žãªãƒ‘ã‚¹ã®ã‚¿ã‚¤ãƒ—ã§ã™: {file_or_dir_path}[/red]")
        raise typer.Exit(code=1)


def _validate_file(
    file_path: str,
    schema_path_str: str | None,
    encoding: str | None,
) -> None:
    schema_path = (
        Path(schema_path_str) if schema_path_str else Path(file_path + ".schema.yaml")
    )
    validate_paths(file_path, schema_path)
    read_data_with_schema(file_path, schema_path, encoding=encoding)
    rprint("[green]ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒ¼ãƒžã«é©åˆã—ã¦ã„ã¾ã™ã€‚[/green]")


def _validate_directory(
    dir_path: str,
    schema_path_str: str | None,
    encoding: str | None,
) -> None:
    dir_path_obj = Path(dir_path)
    file_paths = list(
        it.chain.from_iterable(
            dir_path_obj.glob(ext) for ext in VALIDATION_TARGET_EXTENSIONS
        ),
    )

    if not file_paths:
        rprint("[yellow]æ¤œè¨¼å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚[/yellow]")
        return

    schema_path = (
        Path(schema_path_str) if schema_path_str else dir_path_obj / "schema.yaml"
    )
    if not schema_path.exists():
        rprint(f"[red]ã‚¹ã‚­ãƒ¼ãƒžãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {schema_path}[/red]")
        raise typer.Exit(code=1)

    with ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(
                read_data_with_schema,
                f.as_posix(),
                schema_path,
                encoding,
                write_hash=False,
                quiet=True,
            ): f
            for f in file_paths
        }
        for future in track(
            as_completed(futures),
            total=len(futures),
            description="æ¤œè¨¼ä¸­",
        ):
            try:
                future.result()
            except (ValidationError, typer.Exit) as e:
                rprint(f"[red]ã‚¨ãƒ©ãƒ¼: {e}[/red]")


# --- infer-schema command --- #
@app.command(
    name="infer-schema",
    help="ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¹ã‚­ãƒ¼ãƒžå®šç¾©(.schema.yaml)ã‚’å¯¾è©±çš„ã«ç”Ÿæˆã—ã¾ã™ã€‚",
)
def infer_schema(
    file_path: Annotated[str, typer.Argument(help="ã‚¹ã‚­ãƒ¼ãƒžã‚’æŽ¨æ¸¬ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")],
    encoding: Annotated[
        str | None,
        typer.Option(help="ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆä¾‹: utf-8, cp932 ãªã©ï¼‰"),
    ] = None,
) -> None:
    path = Path(file_path)
    if not path.exists():
        rprint(f"[red]ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}[/red]")
        raise typer.Exit(code=1)

    file_type = infer_file_type(file_path)
    if not file_type:
        rprint("[red]ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’æŽ¨æ¸¬ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚[/red]")
        raise typer.Exit(code=1)

    enc = encoding if encoding else detect_encoding(file_path)

    try:
        if file_type == "text/csv":
            input_dataframe = pl.read_csv(file_path, try_parse_dates=True, encoding=enc)
        elif file_type in (
            "application/vnd.ms-excel",
            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        ):
            input_dataframe = pl.read_excel(file_path)
        elif file_type == "application/json":
            input_dataframe = pl.read_json(file_path)
        elif file_type == "application/x-parquet":
            input_dataframe = pl.read_parquet(file_path)
        else:
            msg = f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: {file_type}"
            raise NotImplementedError(msg)

        jst_year = datetime.datetime.now(
            tz=datetime.timezone(datetime.timedelta(hours=9)),
        ).year
        schema = DatasetConfig(
            name=LocalizedStr(
                ja=typer.prompt("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åç§°ï¼ˆæ—¥æœ¬èªžï¼‰"),
                en=typer.prompt("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åç§°ï¼ˆè‹±èªžï¼‰"),
            ),
            description=LocalizedStr(
                ja=typer.prompt("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª¬æ˜Žï¼ˆæ—¥æœ¬èªžï¼‰"),
                en=typer.prompt("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª¬æ˜Žï¼ˆè‹±èªžï¼‰"),
            ),
            license_=LocalizedStr(
                ja=typer.prompt(
                    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ï¼ˆæ—¥æœ¬èªžï¼‰",
                    default=f"{jst_year}å¹´åº¦ã®è¡Œå‹•ãƒ¢ãƒ‡ãƒ«å¤ã®å­¦æ ¡ã®ãŸã‚ã®åˆ©ç”¨ã®ã¿è¨±å¯ã—ã¾ã™ã€‚",
                ),
                en=typer.prompt(
                    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ï¼ˆè‹±èªžï¼‰",
                    default=f"Use is permitted only for the {jst_year} Summer Course on Behavior Modeling in Transportation Networks.",
                ),
            ),
            city=LocalizedStr(
                ja=typer.prompt("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®éƒ½å¸‚ï¼ˆæ—¥æœ¬èªžï¼‰"),
                en=typer.prompt("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®éƒ½å¸‚ï¼ˆè‹±èªžï¼‰"),
            ),
            year=typer.prompt("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¹´", type=int),
            columns=[
                ColumnConfig(
                    name=name,
                    type_=get_polars_data_type_name(dtype),
                    description=None,
                )
                for name, dtype in zip(
                    input_dataframe.columns,
                    input_dataframe.schema.values(),
                    strict=True,
                )
            ],
            hash_=None,
        )

        schema_path = path.with_suffix(path.suffix + ".schema.yaml")
        if schema_path.exists() and not typer.confirm(
            "æ—¢å­˜ã®ã‚¹ã‚­ãƒ¼ãƒžãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ãã—ã¾ã™ã‹ï¼Ÿ",
            default=False,
        ):
            rprint("[red]ã‚¹ã‚­ãƒ¼ãƒžã®æŽ¨æ¸¬ã‚’ä¸­æ­¢ã—ã¾ã—ãŸã€‚[/red]")
            raise typer.Exit(code=1)

        schema_path.parent.mkdir(parents=True, exist_ok=True)
        with schema_path.open("w", encoding="utf-8") as f:
            yaml.dump(
                schema.model_dump(),
                f,
                indent=4,
                allow_unicode=True,
                sort_keys=False,
            )

        rprint(f"[green]ã‚¹ã‚­ãƒ¼ãƒžã‚’ {schema_path} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚[/green]")

    except (ValueError, pl.PolarsError) as e:
        rprint(f"[red]ã‚¹ã‚­ãƒ¼ãƒžã®æŽ¨æ¸¬ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}[/red]")
        raise typer.Exit(code=1) from e


# --- generate-pdf command --- #
@app.command(
    name="generate-pdf",
    help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®å…¨ã¦ã®å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰PDFã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
)
def generate_pdf(
    root_path: Annotated[
        str,
        typer.Argument(help="ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹"),
    ] = ".",
) -> None:
    from dataset_cli.utils.pdf import create_readme_pdf  # noqa: PLC0415

    repo_path = Path(root_path)
    data_dir = repo_path / "data"

    if not data_dir.is_dir():
        rprint("[yellow]è­¦å‘Š: 'data' ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚[/yellow]")
        raise typer.Exit(0)

    files_for_pdf = [
        p
        for p in data_dir.rglob("*")
        if p.is_file() and p.suffix in PDF_TARGET_EXTENSIONS
    ]

    if not files_for_pdf:
        rprint("  [dim]PDFç”Ÿæˆå¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚[/dim]")
        return

    rprint(f"  [cyan]å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°:[/cyan] {len(files_for_pdf)}å€‹")
    success_count = 0
    failure_count = 0

    for file_path in files_for_pdf:
        try:
            for lang in ("ja", "en"):
                create_readme_pdf(file_path=file_path, lang=lang)
            rprint(f"  [green]âœ“[/green] ç”Ÿæˆå®Œäº†: {file_path.relative_to(repo_path)}")
            success_count += 1
        except (typer.Exit, FileNotFoundError, ValidationError) as e:
            rprint(
                f"[bold red]  âœ—[/bold red] ç”Ÿæˆå¤±æ•—: {file_path.relative_to(repo_path)}",
            )
            rprint(f"    [dim]ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}[/dim]")
            failure_count += 1

    rprint("\n[bold]å®Œäº†[/bold]")
    rprint(f"  [green]æˆåŠŸ:[/green] {success_count} ãƒ•ã‚¡ã‚¤ãƒ« ({success_count * 2} PDF)")
    if failure_count > 0:
        rprint(f"  [red]å¤±æ•—:[/red] {failure_count} ãƒ•ã‚¡ã‚¤ãƒ«")
